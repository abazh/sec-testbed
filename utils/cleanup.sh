#!/bin/bash

# Security Testbed Cleanup Script
# Comprehensive cleanup for all data generated by the security testbed
# 
# Updated for current setup:
# - Suricata 8.0.0 RELEASE IDS/IPS with eve.json ML logging
# - OWASP Juice Shop v15.3.0 vulnerable web application  
# - Machine Learning feature extraction and Random Forest datasets
#
# Handles cleanup of:
# - Suricata logs: eve.json, stats.log, fast.log, suricata.log
# - OWASP Juice Shop logs: juiceshop.log, startup.log
# - ML datasets: ml_features.csv and analysis results
# - OVS switch logs and attacker logs

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
DATA_DIR="$PROJECT_ROOT/data"
DAYS=${1:-7}  # Default: keep files from last 7 days

show_help() {
    echo "Security Testbed Cleanup Script"
    echo "Usage: $0 [days] [--force]"
    echo ""
    echo "Arguments:"
    echo "  days: Keep files newer than N days (default: 7)"
    echo "  --force: Skip confirmation prompts"
    echo ""
    echo "Examples:"
    echo "  $0           # Clean files older than 7 days"
    echo "  $0 3         # Clean files older than 3 days"
    echo "  $0 0         # Clean all generated files"
    echo "  $0 7 --force # Clean without confirmation"
    echo ""
    echo "Cleaned directories:"
    echo "  - data/captures/     # Suricata logs (eve.json, fast.log, stats.log, suricata.log)"
    echo "  - data/analysis/     # ML datasets (.csv, .json) and analysis results"
    echo "  - data/attacker_logs/# Attack logs and markers"
    echo "  - data/switch_logs/  # OVS network switch logs"
    echo "  - data/victim_logs/  # OWASP Juice Shop logs (juiceshop.log, startup.log)"
}

# Parse arguments
FORCE=false
while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_help
            exit 0
            ;;
        --force)
            FORCE=true
            shift
            ;;
        [0-9]*)
            DAYS=$1
            shift
            ;;
        *)
            echo "Error: Unknown argument '$1'"
            show_help
            exit 1
            ;;
    esac
done

# Validate arguments
if ! [[ "$DAYS" =~ ^[0-9]+$ ]]; then
    echo "Error: Days must be a number"
    show_help
    exit 1
fi

if [ ! -d "$DATA_DIR" ]; then
    echo "Error: Data directory not found: $DATA_DIR"
    echo "Make sure you're running this from the testbed root or utils/ directory"
    exit 1
fi

# Calculate total size before cleanup
echo "=== Security Testbed Cleanup ==="
echo "Target directory: $DATA_DIR"
echo "Cleaning files older than $DAYS days"
echo ""

if command -v du >/dev/null 2>&1; then
    INITIAL_SIZE=$(du -sh "$DATA_DIR" 2>/dev/null | cut -f1)
    echo "Current data directory size: $INITIAL_SIZE"
else
    echo "Current data directory size: Unable to calculate"
fi
echo ""

# Show what will be cleaned
echo "Files to be cleaned:"
FOUND_FILES=false

# Check captures directory (Suricata 8.0.0 logs)
if [ -d "$DATA_DIR/captures" ]; then
    if [ "$DAYS" -eq 0 ]; then
        CAPTURE_COUNT=$(find "$DATA_DIR/captures" \( -name "*.json" -o -name "*.log" -o -name "*.pcap*" -o -name "*.arg" \) -type f 2>/dev/null | wc -l)
    else
        CAPTURE_COUNT=$(find "$DATA_DIR/captures" \( -name "*.json" -o -name "*.log" -o -name "*.pcap*" -o -name "*.arg" \) -type f -mtime +$DAYS 2>/dev/null | wc -l)
    fi
    if [ "$CAPTURE_COUNT" -gt 0 ]; then
        echo "  � $CAPTURE_COUNT Suricata 8.0.0 log files in captures/ (eve.json, stats.log, fast.log, suricata.log)"
        FOUND_FILES=true
    fi
fi

# Check analysis directory (ML features and datasets)
if [ -d "$DATA_DIR/analysis" ]; then
    if [ "$DAYS" -eq 0 ]; then
        ANALYSIS_COUNT=$(find "$DATA_DIR/analysis" \( -name "*.csv" -o -name "*.json" -o -name "*.py" \) -type f ! -name "README.md" 2>/dev/null | wc -l)
    else
        ANALYSIS_COUNT=$(find "$DATA_DIR/analysis" \( -name "*.csv" -o -name "*.json" -o -name "*.py" \) -type f ! -name "README.md" -mtime +$DAYS 2>/dev/null | wc -l)
    fi
    if [ "$ANALYSIS_COUNT" -gt 0 ]; then
        echo "  🤖 $ANALYSIS_COUNT ML analysis files in analysis/ (ml_features.csv, ml_demo.py)"
        FOUND_FILES=true
    fi
fi

# Check log directories
for LOG_DIR in "attacker_logs" "switch_logs" "victim_logs"; do
    if [ -d "$DATA_DIR/$LOG_DIR" ]; then
        if [ "$DAYS" -eq 0 ]; then
            LOG_COUNT=$(find "$DATA_DIR/$LOG_DIR" -name "*.log" -type f 2>/dev/null | wc -l)
        else
            LOG_COUNT=$(find "$DATA_DIR/$LOG_DIR" -name "*.log" -type f -mtime +$DAYS 2>/dev/null | wc -l)
        fi
        if [ "$LOG_COUNT" -gt 0 ]; then
            echo "  📝 $LOG_COUNT log files in $LOG_DIR/"
            FOUND_FILES=true
        fi
    fi
done

if [ "$FOUND_FILES" = false ]; then
    echo "  ✅ No files found matching cleanup criteria"
    exit 0
fi

echo ""

# Confirmation prompt
if [ "$FORCE" = false ]; then
    read -p "Continue with cleanup? [y/N]: " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo "Cleanup cancelled"
        exit 0
    fi
fi

echo "🧹 Starting cleanup..."

# Function to safely clean directory (with exclusions for important files)
clean_directory() {
    local dir="$1"
    local pattern="$2"
    local description="$3"
    local exclude_pattern="$4"  # Optional exclusion pattern
    
    if [ ! -d "$dir" ]; then
        return 0
    fi
    
    # For days=0, find all files; otherwise find files older than specified days
    local find_cmd
    if [ "$DAYS" -eq 0 ]; then
        if [ -n "$exclude_pattern" ]; then
            find_cmd="find \"$dir\" -name \"$pattern\" -type f ! -name \"$exclude_pattern\""
        else
            find_cmd="find \"$dir\" -name \"$pattern\" -type f"
        fi
    else
        if [ -n "$exclude_pattern" ]; then
            find_cmd="find \"$dir\" -name \"$pattern\" -type f ! -name \"$exclude_pattern\" -mtime +$DAYS"
        else
            find_cmd="find \"$dir\" -name \"$pattern\" -type f -mtime +$DAYS"
        fi
    fi
    
    local count_before=$(eval "$find_cmd" 2>/dev/null | wc -l)
    if [ "$count_before" -eq 0 ]; then
        return 0
    fi
    
    echo "  Cleaning $description in $dir..."
    
    # Remove files (with exclusions)
    if [ "$DAYS" -eq 0 ]; then
        if [ -n "$exclude_pattern" ]; then
            find "$dir" -name "$pattern" -type f ! -name "$exclude_pattern" -delete 2>/dev/null || true
        else
            find "$dir" -name "$pattern" -type f -delete 2>/dev/null || true
        fi
    else
        if [ -n "$exclude_pattern" ]; then
            find "$dir" -name "$pattern" -type f ! -name "$exclude_pattern" -mtime +$DAYS -delete 2>/dev/null || true
        else
            find "$dir" -name "$pattern" -type f -mtime +$DAYS -delete 2>/dev/null || true
        fi
    fi
    
    local count_after
    if [ -n "$exclude_pattern" ]; then
        count_after=$(find "$dir" -name "$pattern" -type f ! -name "$exclude_pattern" 2>/dev/null | wc -l)
    else
        count_after=$(find "$dir" -name "$pattern" -type f 2>/dev/null | wc -l)
    fi
    local cleaned=$((count_before - count_after))
    
    if [ "$cleaned" -gt 0 ]; then
        echo "    ✅ Removed $cleaned files"
    else
        echo "    ℹ️  No files removed"
    fi
}

# Clean Suricata 8.0.0 logs and packet captures
clean_directory "$DATA_DIR/captures" "*.json" "Suricata eve.json logs"
clean_directory "$DATA_DIR/captures" "*.log" "Suricata log files"
clean_directory "$DATA_DIR/captures" "*.pcap*" "packet captures"
clean_directory "$DATA_DIR/captures" "*.arg" "argus flow files"

# Clean ML analysis outputs (but preserve README.md and important scripts)
clean_directory "$DATA_DIR/analysis" "*.csv" "ML dataset CSV files"
clean_directory "$DATA_DIR/analysis" "*.json" "analysis JSON reports"
clean_directory "$DATA_DIR/analysis" "*.py" "generated Python scripts" "ml_demo.py"

# Clean log files from all log directories
clean_directory "$DATA_DIR/attacker_logs" "*.log" "attacker logs"
clean_directory "$DATA_DIR/switch_logs" "*.log" "OVS switch logs" 
clean_directory "$DATA_DIR/victim_logs" "*.log" "OWASP Juice Shop logs"

# Clean temporary and misc files
for subdir in "captures" "analysis" "attacker_logs" "switch_logs" "victim_logs"; do
    if [ -d "$DATA_DIR/$subdir" ]; then
        echo "  Cleaning temporary files in $subdir..."
        if [ "$DAYS" -eq 0 ]; then
            find "$DATA_DIR/$subdir" \( -name "*.tmp" -o -name "*.temp" -o -name "core" -o -name "*.pid" \) -type f -delete 2>/dev/null || true
        else
            find "$DATA_DIR/$subdir" \( -name "*.tmp" -o -name "*.temp" -o -name "core" -o -name "*.pid" \) -type f -mtime +$DAYS -delete 2>/dev/null || true
        fi
    fi
done

# Remove empty directories (but keep structure)
echo "  Removing empty subdirectories..."
find "$DATA_DIR" -type d -empty -not -path "$DATA_DIR" -not -name "ojs" -not -name "wordpress" -delete 2>/dev/null || true

echo ""
echo "✅ Cleanup completed!"

# Show final size
if command -v du >/dev/null 2>&1; then
    FINAL_SIZE=$(du -sh "$DATA_DIR" 2>/dev/null | cut -f1)
    echo "Final data directory size: $FINAL_SIZE"
else
    echo "Final data directory size: Unable to calculate"
fi

# Show remaining files summary
echo ""
echo "📊 Remaining files summary:"
for subdir in "captures" "analysis" "attacker_logs" "switch_logs" "victim_logs"; do
    if [ -d "$DATA_DIR/$subdir" ]; then
        file_count=$(find "$DATA_DIR/$subdir" -type f 2>/dev/null | wc -l)
        if [ "$file_count" -gt 0 ]; then
            echo "  $subdir/: $file_count files"
        fi
    fi
done
